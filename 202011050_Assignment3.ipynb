{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "202011050_Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJXrLwmNgRc0"
      },
      "source": [
        "# Assignment 3 : Document Retrieval and Evaluation\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6UDvqCY09EG",
        "outputId": "5eebea09-6936-4fbb-d376-7062b637f8fc"
      },
      "source": [
        "#import all needed libraries here\r\n",
        "\r\n",
        "import os\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import lxml\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer \r\n",
        "import numpy as np\r\n",
        "import pickle as pc\r\n",
        "import math\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpScXzq6P-rf",
        "outputId": "0d749bca-9470-4feb-b2ce-a4b49077d237"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mg25cvOk8QR"
      },
      "source": [
        "!unrar x \"/content/drive/MyDrive/FIRE_Dataset_EN_2010.rar\" \"/content/fire_dataset/\"\r\n",
        "# !tar -xvf \"/content/fire_dataset/English-Data.tgz\" -C \"/content\"\r\n",
        "# !cp \"/content/fire_dataset/queries.txt\" \"/content\"\r\n",
        "# !cp \"/content/fire_dataset/relevance.txt\" \"/content\"\r\n",
        "\r\n",
        "import shutil\r\n",
        "shutil.unpack_archive(\"/content/fire_dataset/FIRE_Dataset_EN_2010/English-Data.tgz\", \"/content/fire_dataset/FIRE_Dataset_EN_2010/\")\r\n",
        "\r\n",
        "qrels_path=\"/content/drive/MyDrive/en.qrels.76-125.2010.txt\"\r\n",
        "topics_path=\"/content/fire_dataset/FIRE_Dataset_EN_2010/en.topics.76-125.2010.txt\"\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lQpIM3ilItC"
      },
      "source": [
        "# Function to read all docs \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLqCHa_JnmvL"
      },
      "source": [
        "def read_doc(path):\n",
        "  global docs\n",
        "  global docs_list\n",
        "  for file in os.listdir(path):\n",
        "    if os.path.isdir(os.path.join(path,file)):\n",
        "      read_doc(os.path.join(path,file))\n",
        "    else:\n",
        "      if file.find(\"index\") == -1:\n",
        "        fi = open(os.path.join(path,file),'r')\n",
        "        bs = BeautifulSoup(fi.read())\n",
        "        doc_id = bs.find(\"docno\").text\n",
        "        global i \n",
        "        i += 1\n",
        "        index[doc_id] = i\n",
        "        docs_list.append(doc_id)\n",
        "        docs.append(bs.find(\"text\").text)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lelKFYXaZH1w"
      },
      "source": [
        "##calling read_doc function to read whole corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFmpgqs313Qo"
      },
      "source": [
        "global i \n",
        "index = {}\n",
        "i = -1\n",
        "docs = []\n",
        "q_id = []\n",
        "docs_list = []\n",
        "read_doc(\"/content/fire_dataset/FIRE_Dataset_EN_2010/TELEGRAPH_UTF8\")\n",
        "fi = open(topics_path,'r')\n",
        "data = fi.read()\n",
        "bs = BeautifulSoup(data)\n",
        "q_id = [int(id.text) for id in bs.find_all(\"num\")]   # getting query id\n",
        "desc = [desc.text for desc in bs.find_all(\"desc\")]  # getting description of query\n",
        " \n",
        "docs.extend(desc)\n",
        " \n",
        "for id in q_id:\n",
        "  i += 1\n",
        "  index[id] = i"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa8uS6SzlscI"
      },
      "source": [
        "# Class for preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVMX_GGj6cI4"
      },
      "source": [
        "class preprocessing:\n",
        "  def __init__(self):\n",
        "    self.docs_tokenized = [] # stores tokens of each doc\n",
        "    self.words = set()\n",
        "  def preprocess(self,docs):\n",
        "    temp = []\n",
        "    for doc in docs:\n",
        "      doc = re.sub('(\\d|[^\\w\\s])','',doc) #removing punctuations and digits\n",
        "      doc = nltk.word_tokenize(doc.lower()) #tokenizing\n",
        "      sw = set(stopwords.words('english'))\n",
        "      doc = [token for token in doc  if token not in sw]# removing stop words\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "      doc = [lemmatizer.lemmatize(x) for x in doc] #lemmitizing\n",
        "      self.docs_tokenized.append(doc)\n",
        "      temp.extend(doc)\n",
        "    self.words = set(temp)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lChZtCpA8i3r"
      },
      "source": [
        "ob1 = preprocessing() \n",
        "ob1.preprocess(docs)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys42WFllnc9b"
      },
      "source": [
        "# **Creating list of dictionaries where each dictionary contents word count of that document**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mP-TRWZgyQw"
      },
      "source": [
        "docs_dict = []    \n",
        "for doc in ob1.docs_tokenized:\n",
        "  temp_dict = {}\n",
        "  for word in doc:\n",
        "    temp_dict[word]=temp_dict.get(word,0)+1  #counting the frequency of each word\n",
        "  docs_dict.append(temp_dict)    "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ST58v6LnlpN"
      },
      "source": [
        "#**Creating document frequency of each word which means how many documents have that word.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du3QCfJrnh3I"
      },
      "source": [
        "df = {} #dictionary to store df\r\n",
        "for doc in ob1.docs_tokenized:\r\n",
        "  for word in set(doc):\r\n",
        "    df[word] = df.get(word,0)+1  #Counting numbers of documents which contains specific word\r\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXe_6XNan6rQ"
      },
      "source": [
        "#**Calculating inverse document frequency-IDF for each word**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CofUtKlCnkoL"
      },
      "source": [
        "idf = {}\r\n",
        "N = len(docs_dict)\r\n",
        "for word in df:\r\n",
        "  idf[word] = math.log(N/df[word])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBLwynMuobtK"
      },
      "source": [
        "# **Class for finding tfidf vector**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJrrdf8hgyT9"
      },
      "source": [
        "class tf_idf:\n",
        "  def __init__(self,doc_index,idf,docs_dict,words):\n",
        "    self.doc_index=doc_index\n",
        "    self.idf = idf\n",
        "    self.docs_dict = docs_dict\n",
        "    self.words = words\n",
        "  \"\"\"It returns a dictionary for document which contains word as key and it's Tf-idf score as value \"\"\"  \n",
        "  def getvectordict(self,doc_id):\n",
        "    vector = {}\n",
        "    doc = self.docs_dict[self.doc_index[doc_id]]  #Getting doc from docs dictionary with help of index\n",
        "    n = sum(doc.values())\n",
        "    if n!= 0:\n",
        "      for word in doc:\n",
        "        tf = doc[word] / n                        #Calculating tf\n",
        "        tfidf = tf*self.idf[word]                 #Calculating tf-idf\n",
        "        if tfidf!=0:\n",
        "         vector[word] = tfidf\n",
        "    return vector   \n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoBgrR5bs-OT"
      },
      "source": [
        "ob2 = tf_idf(index,idf,docs_dict,ob1.words)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw4Wp9UUmTgP"
      },
      "source": [
        "#**Find tfidf vectors of queries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtY3Z--ht23g"
      },
      "source": [
        "queries_tfidf = {}\r\n",
        "for q in q_id:\r\n",
        "    queries_tfidf[q] = ob2.getvectordict(q)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-Q2871Zoo54"
      },
      "source": [
        "#**Find tfidf vectors of documnets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ESYBn8-t33L"
      },
      "source": [
        "docs_tfidf = {}\r\n",
        "for doc in docs_list:\r\n",
        "    docs_tfidf[doc] = ob2.getvectordict(doc)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYXBdpsGotgV"
      },
      "source": [
        "# **Calculating cosine similarity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMR6ENKs9lGw"
      },
      "source": [
        "from numpy.linalg import norm\r\n",
        "def c_s(a,b):\r\n",
        "  dot_pr = 0\r\n",
        "  av = list(a.values())\r\n",
        "  bv = list(b.values())\r\n",
        "  for word in a:\r\n",
        "    if word in b:\r\n",
        "      dot_pr = dot_pr + (a[word] * b[word])\r\n",
        "  return (dot_pr/(norm(av)*norm(bv)))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm2n638p7Nxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c142bd-32ed-466f-b3ce-11ec2ee4d8f6"
      },
      "source": [
        "cs = {}\n",
        "for q in q_id:                    # iterating through each query\n",
        "  qv = queries_tfidf[q]           # getting tfidf of specific query\n",
        "  temp = {}\n",
        "  for doc in docs_list:           # calculating cosine similarity of query and document\n",
        "    dv = docs_tfidf[doc]    \n",
        "    similarity = c_s(qv,dv)\n",
        "    if  similarity != 0:               \n",
        "     temp[doc] = similarity          \n",
        "  cs[q] = temp"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIFG96soCrgJ"
      },
      "source": [
        "#**Getting top 10 documents for each query on basis of there cosine similarity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5PWwKn4vO4P"
      },
      "source": [
        "from heapq import nlargest\r\n",
        "rank_list = {}\r\n",
        "for q in q_id:\r\n",
        "  rank_list[q] = nlargest(10,cs[q],key = cs[q].get)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q_29Sx9rg9n"
      },
      "source": [
        "# **Reading qrels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7P0IxY3OH5K"
      },
      "source": [
        "fr = open(qrels_path,'r')\n",
        "qr = {}\n",
        "for line in fr.readlines():\n",
        "  data = line.split()\n",
        "  if data[0] not in qr:\n",
        "   qr[data[0]] = {}\n",
        "   qr[data[0]][data[2]] = int(data[3])          \n",
        "  else: \n",
        "    qr[data[0]][data[2]] = int(data[3])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9p9JGi2tHJG"
      },
      "source": [
        "# **Calculating Mean Average Precision**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOlHbseC-vvX",
        "outputId": "f474dac7-b05b-48ef-ee10-7ab28bdca0d5"
      },
      "source": [
        "avg_precision = []                                \r\n",
        "for q in q_id:                                             #Iterating through each query\r\n",
        "  rk = rank_list[q]                                        # Getting rank list of specific query\r\n",
        "  relevance = 0                                            # Store number of relevant documents found\r\n",
        "  count = 0                                                #To store count of docs\r\n",
        "  precision_sum = 0 \r\n",
        "  for doc in rk: \r\n",
        "    count += 1                         \r\n",
        "    if doc in qr[str(q)]:\r\n",
        "      if qr[str(q)][doc] == 1:                             #Checking if it's relavant document \r\n",
        "        relevance += 1                          \r\n",
        "        precision_sum += (relevance/count)                 #Calculating precision and adding it to local precision_sum                    \r\n",
        "  if relevance==0:\r\n",
        "    avg_precision.append(0)     \r\n",
        "  else:\r\n",
        "    avg_precision.append(precision_sum/relevance)\r\n",
        "  \r\n",
        "map = sum(avg_precision)/len(avg_precision)      \r\n",
        "\r\n",
        "print(\"MAP : \",round(map,2))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAP :  0.49\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}