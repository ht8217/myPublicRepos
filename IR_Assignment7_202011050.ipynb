{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_Assignment7_202011050.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qauk41cYYS5y"
      },
      "source": [
        "# **Assignment 7 : Text Classification**\n",
        "### **Krishna Savaliya(202011050)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgCOoAdGhDLN",
        "outputId": "fa9c8dd2-c9fa-4a27-d3fe-4376bb8c5148"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iqvhw35kjft"
      },
      "source": [
        "#unzip the dataset to the content\n",
        "# dataset7 : sample_submission.csv, train.csv, test.csv, test_labels.csv\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('/content/drive/MyDrive/dataset7.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in different directory\n",
        "   zipObj.extractall('/content/')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJMdnnuap8US",
        "outputId": "9d58f82d-b392-4c11-976c-e29efa55b3cd"
      },
      "source": [
        "#Imported required libraries here\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import glob\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pickle\n",
        "import collections\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZccpAiRXq1x"
      },
      "source": [
        "## Preprocessing of Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go4_GmwCqzjE"
      },
      "source": [
        "class Preprocessing_Data: \n",
        "  def load_data(self, type_of_data):      #To Load the Data Available for applying preprocessing on it\n",
        "    global total\n",
        "    total = 0     \n",
        "    data = list()         #contains list of list of data for each file. each list within list has one corresponding file data\n",
        "    if type_of_data == \"train\":\n",
        "      location = \"/content/dataset7/train.csv\"\n",
        "    else:\n",
        "      location = \"/content/dataset7/test.csv\"        \n",
        "    df = pd.read_csv(location)    #reading csv file \n",
        "    df['comment_text'].replace('', np.nan, inplace=True) \n",
        "    df.dropna(subset=['comment_text'], inplace=True)\n",
        "    if type_of_data == \"train\":\n",
        "      pickle.dump(df, open('/content/drive/MyDrive/IR/dump/train_data','wb'))\n",
        "    else:\n",
        "      pickle.dump(df, open('/content/drive/MyDrive/IR/dump/test_data/test_data_main','wb'))\n",
        "      test_labels = pd.read_csv(\"/content/dataset7/test_labels.csv\")\n",
        "      test_labels = pd.merge(test_labels, df, on = \"id\", how = \"right\")\n",
        "      pickle.dump(test_labels, open('/content/drive/MyDrive/IR/dump/test_data/test_labels_data','wb'))\n",
        "      print(test_labels.shape)\n",
        "      \n",
        "    comments = list(df.loc[:,\"comment_text\"])\n",
        "    comment_ids = list(df.loc[:,\"id\"])\n",
        "    new_comment_ids = list()\n",
        "    for comment_id,comment in zip(comment_ids,comments):                               \n",
        "      data.append(comment)       \n",
        "      new_comment_ids.append(comment_id)\n",
        "      total += 1\n",
        "    return new_comment_ids,data\n",
        "\n",
        "  #function to preprocess the data\n",
        "  def preprocess(self,data,type_of_data):\n",
        "      #retrieving each doc data and then removing numerical and punctuation from each document\n",
        "      stop_words = stopwords.words('english')\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "      preprocessed_data = list()\n",
        "      if type_of_data == \"train\":\n",
        "        xdata = pickle.load(open('/content/drive/MyDrive/IR/dump/train_data','rb'))\n",
        "      else:\n",
        "        xdata = pickle.load(open('/content/drive/MyDrive/IR/dump/test_data/test_data_main','rb'))\n",
        "        test_labels = pickle.load(open('/content/drive/MyDrive/IR/dump/test_data/test_labels_data','rb'))\n",
        "      drop_index = list()\n",
        "      for index,doc_text in zip(xdata.index, data):\n",
        "        #remooving numerical and punctuation\n",
        "        doc_text = re.sub(r'[^\\w\\s]', '',doc_text)\n",
        "        doc_text = re.sub('\\d','',doc_text)\n",
        "        #data tokenizing\n",
        "        data_tokenized = nltk.word_tokenize(doc_text)\n",
        "        #converting to smallcase \n",
        "        data_smallcase = [word.lower() for word in data_tokenized] \n",
        "        #removing stopwords\n",
        "        data_without_stopwords = list()       \n",
        "        for word in data_smallcase:\n",
        "          if word not in stop_words:\n",
        "            data_without_stopwords.append(word)\n",
        "        #contractions\n",
        "        data_contractions = map(expand_contractions, data_without_stopwords)\n",
        "        #Lemmatization\n",
        "        prev_data = list() \n",
        "        for word in data_contractions:\n",
        "          prev_data.append(lemmatizer.lemmatize(word,pos='v'))\n",
        "        if type_of_data == \"train\":\n",
        "          test_row = [\"dummy\"]\n",
        "          test_type = [\"should not be equal\"] \n",
        "        else:\n",
        "          test_row = list(test_labels.loc[index,[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]])\n",
        "          test_type= [-1]*6\n",
        "          \n",
        "        if len(prev_data) == 0 or (test_row == test_type):\n",
        "          drop_index.append(index)\n",
        "        else:\n",
        "          preprocessed_data.append(prev_data)\n",
        "         \n",
        "      df = xdata.drop(drop_index)\n",
        "      if type_of_data == \"train\":\n",
        "        pickle.dump(df,open(\"/content/drive/MyDrive/IR/dump/train_data\",\"wb\"))\n",
        "      else:\n",
        "        test_labels = pd.merge(test_labels,df, on = \"id\", how = \"right\")\n",
        "        pickle.dump(test_labels, open('/content/drive/MyDrive/IR/dump/test_data/test_labels_data','wb'))\n",
        "        print(test_labels.shape)\n",
        "        pickle.dump(df,open(\"/content/drive/MyDrive/IR/dump/test_data/test_data_main\",\"wb\"))\n",
        "        \n",
        "      return preprocessed_data\n",
        "  \n",
        "  def extractUnique(self,data):\n",
        "    #return unique feature from the entire corpus\n",
        "    return collections.Counter([x for sub_list in data for x in sub_list])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdUsq8dNLJSy"
      },
      "source": [
        "#creating preprocessing object class\n",
        "p1 = Preprocessing_Data()\n",
        "#loading the data\n",
        "comment_ids,data = p1.load_data(\"train\")\n",
        "#preprocessing data\n",
        "preprocessed_data = p1.preprocess(data,\"train\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYqduQHRRHfv",
        "outputId": "1a3e1fd4-a2d6-4b97-c39b-aac2a232fda5"
      },
      "source": [
        "#As the data has been preprocessed dumping it so that we do not require to run the preprocessing step again \n",
        "pickle.dump(preprocessed_data, open('/content/drive/MyDrive/IR/dump/comment_preprocessed','wb'))\n",
        "pickle.dump(comment_ids,open('/content/drive/MyDrive/IR/dump/comment_ids','wb'))\n",
        "infile = open('/content/drive/MyDrive/IR/dump/comment_preprocessed','rb')\n",
        "preprocessed_data = pickle.load(infile)\n",
        "infile.close()\n",
        "infile = open('/content/drive/MyDrive/IR/dump/comment_ids','rb')\n",
        "doc = pickle.load(infile)\n",
        "infile.close()\n",
        "total = len(preprocessed_data)\n",
        "print(\"Number of documents=\",total)\n",
        "feature_collection = p1.extractUnique(preprocessed_data)\n",
        "feature_list= list(feature_collection)\n",
        "comment_feature_list = list()\n",
        "for document in preprocessed_data:\n",
        "  c = collections.Counter([word for word in document])\n",
        "  comment_feature_list.append(c)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents= 159525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7HScsfzGnJd"
      },
      "source": [
        "## TF_IDF Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOyPYbU2XjXU"
      },
      "source": [
        "TFIDF class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq3sTRfwzVz-"
      },
      "source": [
        "class TF_IDF:  \n",
        "  def __init__(self):   \n",
        "    self.idf_matrix = csr_matrix(self.find_idf_matrix1())\n",
        "    \n",
        "  #function to fit the data\n",
        "  def fit(self,comments):\n",
        "    global feature_list\n",
        "    #indices for generating sparse matrix \n",
        "    global comment_feature_list\n",
        "    rows = list()\n",
        "    columns = list() \n",
        "    values = list() \n",
        "  \n",
        "    reverse_feature_list = dict()\n",
        "    for i,word in enumerate(feature_list):\n",
        "      reverse_feature_list[word] = i \n",
        "    \n",
        "    for i,document in zip(range(len(comments)),feature_list):\n",
        "      total_words_in_doc = len(document)\n",
        "      #if i%10000 == 0 :\n",
        "        #print(\"document processed=\",i)\n",
        "      \n",
        "      for word in comment_feature_list[i]:\n",
        "        count = comment_feature_list[i][word]\n",
        "        j = reverse_feature_list.get(word,-1)\n",
        "        if j == -1:\n",
        "          continue\n",
        "        rows.append(i)\n",
        "        columns.append(j)\n",
        "        values.append(float(count)/total_words_in_doc)       #term frequency\n",
        "    tf_sparse = csr_matrix((values, (rows, columns)), shape=(len(comments), len(feature_list)))\n",
        "    return csr_matrix.multiply(tf_sparse, self.idf_matrix)       #tf*idf \n",
        "\n",
        "  #function to transform the query to tf-idf representation\n",
        "  def transform(self, test_comments):\n",
        "    global feature_list \n",
        "    global test_comment_feature_list\n",
        "\n",
        "    #indices for generating sparse matrix  \n",
        "    rows = list()\n",
        "    columns = list() \n",
        "    values = list() \n",
        "\n",
        "    reverse_feature_list = dict()\n",
        "    for i,word in enumerate(feature_list):\n",
        "      reverse_feature_list[word] = i\n",
        "    for i, document in zip(range(len(test_comments)),feature_list):\n",
        "      total_words_in_doc = len(document)\n",
        "      for word in test_comment_feature_list[i]:\n",
        "          count = test_comment_feature_list[i][word]\n",
        "          j = reverse_feature_list.get(word,-1)\n",
        "          if j == -1:\n",
        "            continue\n",
        "          element_value = float(count)/total_words_in_doc\n",
        "          rows.append(i)\n",
        "          columns.append(j)\n",
        "          values.append(element_value) \n",
        "    tf_sparse = csr_matrix((values, (rows, columns)), shape=(len(test_comments), len(feature_list)))\n",
        "    return csr_matrix.multiply(tf_sparse,self.idf_matrix)\n",
        "\n",
        "  def return_feature_generator(self):\n",
        "    global feature_list \n",
        "    for ele in feature_list: \n",
        "      yield (ele)\n",
        "      \n",
        "  def find_idf_matrix1(self):\n",
        "    #return list which stores the idf value of each unique term\n",
        "    global feature_list\n",
        "    global feature_collection \n",
        "    global preprocessed_data \n",
        "    temp_matrix = np.zeros((1,len(feature_list)))\n",
        "    feature_list_generator = self.return_feature_generator()\n",
        "    for i,ele in zip(range(len(feature_list)),feature_list_generator):\n",
        "      temp_matrix[0,i] = feature_collection[ele]\n",
        "    return np.log10(np.multiply(total,np.reciprocal(temp_matrix)))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmR6t9RoXbev"
      },
      "source": [
        "TFIDF fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwJadF_h9gCo",
        "outputId": "99e2980f-bd50-4d09-d4f9-bd9d109b4896"
      },
      "source": [
        "#generating object for preprocesssing object as previous generator has already been used \n",
        "tfidf_obj = TF_IDF()\n",
        "term_doc_matrix = tfidf_obj.fit(preprocessed_data)        #passing preprocessed generator\n",
        "print(term_doc_matrix.shape)\n",
        "#generated object for term_doc_matrix generator cannot print shape because the class is generator\n",
        "del feature_collection \n",
        "del comment_feature_list"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(159525, 215397)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odr_LeRiXetF"
      },
      "source": [
        "TFIDF transform comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WE5XjXE-jzK",
        "outputId": "4d00df4a-4d67-4e0d-db24-3696bc1ef4c3"
      },
      "source": [
        "#loading test_comments \n",
        "p2 = Preprocessing_Data()\n",
        "test_comment_ids , test_comments = p2.load_data(\"test\")\n",
        "\n",
        "#Preprocessing the test data\n",
        "test_comment_generator = p2.preprocess(test_comments,\"test\")\n",
        "test_comment_feature_list = list()\n",
        "for comment in test_comment_generator:\n",
        "  c = collections.Counter([word for word in comment])\n",
        "  test_comment_feature_list.append(c)\n",
        "\n",
        "#transforming test_comments\n",
        "transformed_comments = tfidf_obj.transform(test_comment_generator)                           #will contain all the transformed test comments\n",
        "\n",
        "pickle.dump(transformed_comments, open('/content/drive/MyDrive/IR/dump/transformed_comments','wb'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(153164, 8)\n",
            "(63929, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT-gOWN9XM6R"
      },
      "source": [
        "Making function to calculate prision, recall and fscore "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dREhBZYqCVC3"
      },
      "source": [
        "import sklearn\n",
        "#making a utility function that calculates precision, recall and fscore\n",
        "def fscore(average, y_test, y_pred, labels):\n",
        "  return sklearn.metrics.precision_recall_fscore_support(y_test, y_pred, beta=1.0,labels=[labels]\n",
        "                                                ,average=average, warn_for={'precision', 'recall', 'f-score'}\n",
        "                                                ,zero_division='warn')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaFRz1pFW8qj"
      },
      "source": [
        "Using SVM for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6F1dPpyCKN0",
        "outputId": "a0443391-0177-4bc0-ee60-736986cbdfac"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "  \n",
        "# Building a Support Vector Machine on train data so here we are using svm classifier.  \n",
        "svc = LinearSVC()\n",
        "x_train = term_doc_matrix\n",
        "df = pickle.load(open(\"/content/drive/MyDrive/IR/dump/train_data\",\"rb\"))\n",
        "y_train  = np.array(df.loc[:,[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]])\n",
        "svc_multioutput_classifier = MultiOutputClassifier(svc,n_jobs = -1)\n",
        "svc_multioutput_classifier = svc_multioutput_classifier.fit(x_train, y_train)\n",
        "\n",
        "#save the classifier data \n",
        "pickle.dump(svc_multioutput_classifier,open(\"/content/drive/MyDrive/IR/dump/svc_multioutput\",\"wb\"))\n",
        "\n",
        "column_labels = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
        "transformed_comments = pickle.load(open(\"/content/drive/MyDrive/IR/dump/transformed_comments\",\"rb\"))\n",
        "svc_multioutput_classifier = pickle.load(open(\"/content/drive/MyDrive/IR/dump/svc_multioutput\",\"rb\"))\n",
        "y_test = pickle.load(open(\"/content/drive/MyDrive/IR/dump/test_data/test_labels_data\",\"rb\"))\n",
        "y_test = np.array(y_test.loc[:,column_labels])\n",
        "\n",
        "#SVM prediction and result of classification\n",
        "y_pred= svc_multioutput_classifier.predict(transformed_comments)\n",
        "for i in range(len(column_labels)):\n",
        "  print(\"\\nClass : \",column_labels[i])\n",
        "  print(\"\\n\\nMicro : \\n\")\n",
        "  micro_scores = fscore(\"micro\",y_test,y_pred,i)\n",
        "  print(\"Precision score : \",micro_scores[0])\n",
        "  print(\"Recall score : \",micro_scores[1])\n",
        "  print(\"F-score score : \",micro_scores[2])\n",
        "  print(\"\\n\")\n",
        "  print(\"\\nMacro : \\n\")\n",
        "  macro_scores = fscore(\"macro\",y_test,y_pred,i)\n",
        "  print(\"Precision score : \",macro_scores[0])\n",
        "  print(\"Recall score : \",macro_scores[1])\n",
        "  print(\"F-score score : \",macro_scores[2])\n",
        "  print(\"\\n------------------------------------------\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Class :  toxic\n",
            "\n",
            "\n",
            "Micro :\n",
            "\n",
            "Precision score :  0.5493636475966885\n",
            "Recall score :  0.7300492610837438\n",
            "F-score score :  0.6269477543538038\n",
            "\n",
            "\n",
            "\n",
            "Macro :\n",
            "\n",
            "Precision score :  0.5493636475966885\n",
            "Recall score :  0.7300492610837438\n",
            "F-score score :  0.6269477543538038\n",
            "\n",
            "------------------------------------------\n",
            "\n",
            "Class :  severe_toxic\n",
            "\n",
            "\n",
            "Micro :\n",
            "\n",
            "Precision score :  0.3271889400921659\n",
            "Recall score :  0.3869209809264305\n",
            "F-score score :  0.3545568039950063\n",
            "\n",
            "\n",
            "\n",
            "Macro :\n",
            "\n",
            "Precision score :  0.3271889400921659\n",
            "Recall score :  0.3869209809264305\n",
            "F-score score :  0.3545568039950063\n",
            "\n",
            "------------------------------------------\n",
            "\n",
            "Class :  obscene\n",
            "\n",
            "\n",
            "Micro :\n",
            "\n",
            "Precision score :  0.6127314227745372\n",
            "Recall score :  0.6545651584936332\n",
            "F-score score :  0.632957820277705\n",
            "\n",
            "\n",
            "\n",
            "Macro :\n",
            "\n",
            "Precision score :  0.6127314227745372\n",
            "Recall score :  0.6545651584936332\n",
            "F-score score :  0.632957820277705\n",
            "\n",
            "------------------------------------------\n",
            "\n",
            "Class :  threat\n",
            "\n",
            "\n",
            "Micro :\n",
            "\n",
            "Precision score :  0.21818181818181817\n",
            "Recall score :  0.22748815165876776\n",
            "F-score score :  0.22273781902552203\n",
            "\n",
            "\n",
            "\n",
            "Macro :\n",
            "\n",
            "Precision score :  0.21818181818181817\n",
            "Recall score :  0.22748815165876776\n",
            "F-score score :  0.22273781902552203\n",
            "\n",
            "------------------------------------------\n",
            "\n",
            "Class :  insult\n",
            "\n",
            "\n",
            "Micro :\n",
            "\n",
            "Precision score :  0.5234972677595628\n",
            "Recall score :  0.41931718704406185\n",
            "F-score score :  0.465651328580687\n",
            "\n",
            "\n",
            "\n",
            "Macro :\n",
            "\n",
            "Precision score :  0.5234972677595628\n",
            "Recall score :  0.41931718704406185\n",
            "F-score score :  0.465651328580687\n",
            "\n",
            "------------------------------------------\n",
            "\n",
            "Class :  identity_hate\n",
            "\n",
            "\n",
            "Micro :\n",
            "\n",
            "Precision score :  0.36476426799007444\n",
            "Recall score :  0.20646067415730338\n",
            "F-score score :  0.26367713004484306\n",
            "\n",
            "\n",
            "\n",
            "Macro :\n",
            "\n",
            "Precision score :  0.36476426799007444\n",
            "Recall score :  0.20646067415730338\n",
            "F-score score :  0.26367713004484306\n",
            "\n",
            "------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTCUSWC4HD78"
      },
      "source": [
        "## Word2Vec Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwqFdnYsHunE"
      },
      "source": [
        "comment_preprocessed = pickle.load(open(\"/content/drive/MyDrive/IR/dump/comment_preprocessed\",\"rb\"))\n",
        "from gensim.models import Word2Vec\n",
        "skipgram_model = Word2Vec(comment_preprocessed, size=300, window=10, min_count=1, sg=1, workers=4, alpha=0.025, iter=5, negative=0)  #skipgram model of window size = 10 and size = 300"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgq3C55dIaDB"
      },
      "source": [
        "#saving model\n",
        "pickle.dump(skipgram_model,open(\"/content/drive/MyDrive/IR/dump/skipgram_model\",\"wb\"))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DFdKYP_WCpC"
      },
      "source": [
        "Vectorizing training comments:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHeDUmBGIqh_",
        "outputId": "eca8b9dd-564e-4912-b5e7-58420a4a17b4"
      },
      "source": [
        "train_comments = np.zeros((len(comment_preprocessed),300))\n",
        "count = 0\n",
        "for i in comment_preprocessed:\n",
        "  temp = np.zeros((300))\n",
        "  for j in i:\n",
        "    try:\n",
        "      temp = temp + skipgram_model[j]\n",
        "    except:\n",
        "      continue\n",
        "  train_comments[count,:] = temp\n",
        "  count = count + 1 \n",
        "print(\"Length of train array:\",len(train_comments))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of train array: 159525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UrxnYtLWMWk"
      },
      "source": [
        "Vectorizing test comments:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arLZ3LE9IwqA",
        "outputId": "39157af6-7725-43f5-8c3a-ad24468478c7"
      },
      "source": [
        "test_comments = np.zeros((len(test_comment_generator),300)) \n",
        "count = 0\n",
        "for i in test_comment_generator:\n",
        "  temp = np.zeros((300))\n",
        "  for j in i:\n",
        "      try:\n",
        "        temp = temp + skipgram_model[j]\n",
        "      except:\n",
        "        continue\n",
        "  test_comments[count,:] = temp\n",
        "  count = count + 1\n",
        "print(\"Length of test array:\",len(test_comments))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of test array: 63929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klb2tDRHWTKs"
      },
      "source": [
        "Using SVM for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qAXhRRqKBjL",
        "outputId": "d1073769-babb-4c14-83e2-59b4a19c02e6"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "  \n",
        "# Building a Support Vector Machine on train data so here we are using svm classifier.  \n",
        "svc = LinearSVC()\n",
        "x_train = train_comments\n",
        "df = pickle.load(open(\"/content/drive/MyDrive/IR/dump/train_data\",\"rb\"))\n",
        "y_train  = np.array(df.loc[:,[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]])\n",
        "svc_multioutput_classifier = MultiOutputClassifier(svc,n_jobs = -1)\n",
        "svc_multioutput_classifier = svc_multioutput_classifier.fit(x_train, y_train)\n",
        "\n",
        "#save the classifier data \n",
        "pickle.dump(svc_multioutput_classifier,open(\"/content/drive/MyDrive/IR/dump/svc_multioutput_word\",\"wb\"))\n",
        "\n",
        "column_labels = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
        "transformed_comments = test_comments\n",
        "svc_multioutput_classifier = pickle.load(open(\"/content/drive/MyDrive/IR/dump/svc_multioutput_word\",\"rb\"))\n",
        "y_test = pickle.load(open(\"/content/drive/MyDrive/IR/dump/test_data/test_labels_data\",\"rb\"))\n",
        "y_test = np.array(y_test.loc[:,column_labels])\n",
        "\n",
        "#SVM prediction and results of classification\n",
        "y_pred= svc_multioutput_classifier.predict(transformed_comments)\n",
        "for i in range(len(column_labels)):\n",
        "  print(\"\\nClass : \",column_labels[i])\n",
        "  print(\"\\n\\nMicro : \\n\")\n",
        "  micro_scores = fscore(\"micro\",y_test,y_pred,i)\n",
        "  print(\"Precision score : \",micro_scores[0])\n",
        "  print(\"Recall score : \",micro_scores[1])\n",
        "  print(\"F-score score : \",micro_scores[2])\n",
        "  print(\"\\n\")\n",
        "  print(\"\\nMacro : \\n\")\n",
        "  macro_scores = fscore(\"macro\",y_test,y_pred,i)\n",
        "  print(\"Precision score : \",macro_scores[0])\n",
        "  print(\"Recall score : \",macro_scores[1])\n",
        "  print(\"F-score score : \",macro_scores[2])\n",
        "  print(\"\\n-----------------------------------------\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Class :  toxic\n",
            "\n",
            "\n",
            "Micro :\n",
            "\n",
            "Precision score :  0.6542553191489362\n",
            "Recall score :  0.08078817733990148\n",
            "F-score score :  0.143817597193803\n",
            "\n",
            "\n",
            "\n",
            "Macro :\n",
            "\n",
            "Precision score :  0.6542553191489362\n",
            "Recall score :  0.08078817733990148\n",
            "F-score score :  0.143817597193803\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "Class :  severe_toxic\n",
            "\n",
            "\n",
            "Micro :\n",
            "\n",
            "Precision score :  0.43373493975903615\n",
            "Recall score :  0.09809264305177112\n",
            "F-score score :  0.16000000000000003\n",
            "\n",
            "\n",
            "\n",
            "Macro :\n",
            "\n",
            "Precision score :  0.43373493975903615\n",
            "Recall score :  0.09809264305177112\n",
            "F-score score :  0.16000000000000003\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "Class :  obscene\n",
            "\n",
            "\n",
            "Micro :\n",
            "\n",
            "Precision score :  0.7177242888402626\n",
            "Recall score :  0.08886480628555947\n",
            "F-score score :  0.15814850530376084\n",
            "\n",
            "\n",
            "\n",
            "Macro :\n",
            "\n",
            "Precision score :  0.7177242888402626\n",
            "Recall score :  0.08886480628555947\n",
            "F-score score :  0.15814850530376084\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "Class :  threat\n",
            "\n",
            "\n",
            "Micro :\n",
            "\n",
            "Precision score :  0.22727272727272727\n",
            "Recall score :  0.023696682464454975\n",
            "F-score score :  0.04291845493562232\n",
            "\n",
            "\n",
            "\n",
            "Macro :\n",
            "\n",
            "Precision score :  0.22727272727272727\n",
            "Recall score :  0.023696682464454975\n",
            "F-score score :  0.04291845493562232\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "Class :  insult\n",
            "\n",
            "\n",
            "Micro :\n",
            "\n",
            "Precision score :  0.46308724832214765\n",
            "Recall score :  0.020134228187919462\n",
            "F-score score :  0.03859060402684564\n",
            "\n",
            "\n",
            "\n",
            "Macro :\n",
            "\n",
            "Precision score :  0.46308724832214765\n",
            "Recall score :  0.020134228187919462\n",
            "F-score score :  0.03859060402684564\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "Class :  identity_hate\n",
            "\n",
            "\n",
            "Micro :\n",
            "\n",
            "Precision score :  0.36363636363636365\n",
            "Recall score :  0.016853932584269662\n",
            "F-score score :  0.03221476510067114\n",
            "\n",
            "\n",
            "\n",
            "Macro :\n",
            "\n",
            "Precision score :  0.36363636363636365\n",
            "Recall score :  0.016853932584269662\n",
            "F-score score :  0.03221476510067114\n",
            "\n",
            "-----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6K-6AGwKPpO"
      },
      "source": [
        "import re\n",
        "contractions_dict = {\n",
        "'didn\\'t': 'did not',\n",
        "'don\\'t': 'do not',\n",
        "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
        "\"aren't\": \"are not / am not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had / he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he shall / he will\",\n",
        "\"he'll've\": \"he shall have / he will have\",\n",
        "\"he's\": \"he has / he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how has / how is / how does\",\n",
        "\"I'd\": \"I had / I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I shall / I will\",\n",
        "\"I'll've\": \"I shall have / I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had / it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it shall / it will\",\n",
        "\"it'll've\": \"it shall have / it will have\",\n",
        "\"it's\": \"it has / it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had / she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she shall / she will\",\n",
        "\"she'll've\": \"she shall have / she will have\",\n",
        "\"she's\": \"she has / she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as / so is\",\n",
        "\"that'd\": \"that would / that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that has / that is\",\n",
        "\"there'd\": \"there had / there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there has / there is\",\n",
        "\"they'd\": \"they had / they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they shall / they will\",\n",
        "\"they'll've\": \"they shall have / they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had / we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what shall / what will\",\n",
        "\"what'll've\": \"what shall have / what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what has / what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when has / when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where has / where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who shall / who will\",\n",
        "\"who'll've\": \"who shall have / who will have\",\n",
        "\"who's\": \"who has / who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why has / why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had / you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you shall / you will\",\n",
        "\"you'll've\": \"you shall have / you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n",
        "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "def expand_contractions(s, contractions_dict=contractions_dict):  #contraction function\n",
        "    def replace(match):                                              \n",
        "         return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, s)                        #replacing key with by above defined dictionary"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}